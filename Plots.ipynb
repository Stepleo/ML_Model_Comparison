{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for VGG with lr=0.001 in the vanilla setting\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_io.BufferedReader' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m descriptions:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for VGG with lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m setting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     vgg_res \u001b[38;5;241m=\u001b[39m get_dict(common_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epochs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m vgg_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m     t_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(vgg_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mget_dict\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dict\u001b[39m(path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m         d \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[1;32m      6\u001b[0m         f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.BufferedReader' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def get_dict(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "        f.close()\n",
    "    return d\n",
    "\n",
    "epochs = 20\n",
    "lrs = [1e-3, 1e-5]\n",
    "descriptions = [\"vanilla\", \"noisy\"]\n",
    "common_path = \"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/training/\"\n",
    "\n",
    "for lr in lrs:\n",
    "    for d in descriptions:\n",
    "\n",
    "        print(f\"Results for VGG with lr={lr} in the {d} setting\")\n",
    "        vgg_res = get_dict(common_path + f\"VGG_{epochs}_epochs_{lr}_lr_{d}.pkl\")\n",
    "        val_acc = vgg_res[\"validation_accuracy\"][-1]\n",
    "        t_time = sum(vgg_res[\"training_time\"])\n",
    "        print(f\"Validation accuracy : {val_acc}, Total Training time : {t_time}\")\n",
    "\n",
    "        print(f\"Results for Resnet with lr={lr} in the {d} setting\")\n",
    "        resnet_res = get_dict(common_path + f\"ResNet_{epochs}_epochs_{lr}_lr_{d}.pkl\")\n",
    "        val_acc = resnet_res[\"validation_accuracy\"][-1]\n",
    "        t_time = sum(resnet_res[\"training_time\"])\n",
    "        print(f\"Validation accuracy : {val_acc}, Total Training time : {t_time}\")\n",
    "\n",
    "        print(f\"Results for Unet not pretrained with lr={lr} in the {d} setting\")\n",
    "        unet_res = get_dict(common_path + f\"UNet_{epochs}_epochs_{lr}_lr_{d}no_pretraining.pkl\")\n",
    "        val_acc = unet_res[\"validation_accuracy\"][-1]\n",
    "        t_time = sum(unet_res[\"training_time\"])\n",
    "        print(f\"Validation accuracy : {val_acc}, Total Training time : {t_time}\")\n",
    "\n",
    "        print(f\"Results for Unet pretrained with lr={lr} in the {d} setting\")\n",
    "        unet_res = get_dict(common_path + f\"UNet_{epochs}_epochs_{lr}_lr_{d}_pretrained_resnet.pkl\")\n",
    "        val_acc = unet_res[\"validation_accuracy\"][-1]\n",
    "        t_time = sum(unet_res[\"training_time\"])\n",
    "        print(f\"Validation accuracy : {val_acc}, Total Training time : {t_time}\")\n",
    "\n",
    "        print(f\"Results for VAE with lr={lr} in the {d} setting\")\n",
    "        vae_gen_res = get_dict(common_path + f\"VAE_{epochs}_epochs_{lr}_lr_{d}_generation.pkl\")\n",
    "        t_time_gen = sum(vae_gen_res[\"training_time\"])\n",
    "        svm_res = get_dict(common_path + f\"VAE_{epochs}_epochs_{lr}_lr_{d}.pkl\")\n",
    "        svm_accuracy = svm_res[\"validation_accuracy\"][-1]\n",
    "        svm_time = sum(svm_res[\"training_time\"])\n",
    "        kmeans_res = get_dict(common_path + f\"VAE_{d}_kmeans.pkl\")\n",
    "        kmeans_accuracy = kmeans_res[\"validation_accuracy\"][-1]\n",
    "        kmeans_time = kmeans_res[\"training_time\"][-1]\n",
    "        print(f\"With SVM : Validation accuracy : {svm_accuracy}, Total Training time : {t_time_gen + svm_time}\")\n",
    "        print(f\"With KMeans : Validation accuracy : {kmeans_accuracy}, Total Training time : {t_time_gen + kmeans_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ClassComp.utils.visualization import plot_training_metrics, plot_vae_training_metrics\n",
    "\n",
    "lr = 1e-5\n",
    "descriptions = [\"vanilla\", \"noisy\"]\n",
    "for d in descriptions:\n",
    "\n",
    "    vgg_res = get_dict(common_path + f\"VGG_{epochs}_epochs_{lr}_lr_{d}.pkl\")\n",
    "    vgg_res[\"name\"] = \"VGG\"\n",
    "    resnet_res = get_dict(common_path + f\"ResNet_{epochs}_epochs_{lr}_lr_{d}.pkl\")\n",
    "    resnet_res[\"name\"] = \"ResNet\"\n",
    "    unet_res_no_pretraining = get_dict(common_path + f\"UNet_{epochs}_epochs_{lr}_lr_{d}no_pretraining.pkl\")\n",
    "    unet_res_no_pretraining[\"name\"] = \"UNet\"\n",
    "    unet_res_pretrained = get_dict(common_path + f\"UNet_{epochs}_epochs_{lr}_lr_{d}_pretrained_resnet.pkl\")\n",
    "    unet_res_pretrained[\"name\"] = \"UNet pretrained\"\n",
    "    svm_res = get_dict(common_path + f\"VAE_{d}_kmeans.pkl\")\n",
    "    svm_res[\"name\"] = \"VAE SVM head\"\n",
    "    vae_gen_res = get_dict(common_path + f\"VAE_{epochs}_epochs_{lr}_lr_{d}_generation.pkl\")\n",
    "\n",
    "    plot_training_metrics([vgg_res, resnet_res, unet_res_no_pretraining, unet_res_pretrained, svm_res], f\"all_{d}\")\n",
    "    plot_training_metrics([vgg_res, resnet_res, unet_res_no_pretraining, unet_res_pretrained], f\"cnn_{d}\")\n",
    "    plot_vae_training_metrics(vae_gen_res, f\"vae_{d}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ClassComp.models.vgg import VGG\n",
    "from ClassComp.models.resnet import ResNet\n",
    "from ClassComp.models.unet import UNet\n",
    "from ClassComp.data_utils.loaders import get_mnist, get_dataloader\n",
    "\n",
    "# Get binary MNIST subsets\n",
    "train_subset, test_subset = get_mnist()\n",
    "\n",
    "# Get DataLoaders with additional transformations\n",
    "train_loader, test_loader = get_dataloader(train_subset, test_subset, size=64, batch_size=1)\n",
    "\n",
    "# Select a sample from the dataset (one \"0\" and one \"1\")\n",
    "class_zero_sample = None\n",
    "class_one_sample = None\n",
    "\n",
    "# Iterate over the train_loader to find one sample of each class\n",
    "for images, labels in train_loader:\n",
    "    if labels.item() == 0 and class_zero_sample is None:\n",
    "        class_zero_sample = images\n",
    "    elif labels.item() == 1 and class_one_sample is None:\n",
    "        class_one_sample = images\n",
    "    if class_zero_sample is not None and class_one_sample is not None:\n",
    "        break\n",
    "\n",
    "conv_layers = [\"conv_block_1\", \"conv_block_5\"]\n",
    "res_conv_layers = [\"res_conv_block_1\", \"res_conv_block_5\"]\n",
    "descriptions = [\"vanilla\", \"noisy\"]\n",
    "\n",
    "for d in descriptions:\n",
    "    # Load model\n",
    "    vgg = VGG(64, 1)\n",
    "    vgg_path = f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/models/vgg_20_epochs_0.001_lr_{d}.pth.tar\"\n",
    "    vgg.load_state_dict(torch.load(vgg_path))\n",
    "    vgg.eval()\n",
    "\n",
    "\n",
    "    # Visualize Filters\n",
    "    print(\"Visualizing filters of the first convolutional layer:\")\n",
    "    vgg.visualize_filters(\n",
    "            layer_name=conv_layers[0],\n",
    "            inputs=class_zero_sample,\n",
    "            num_filters=10,\n",
    "            save_name=f\"VGG_{conv_layers[0]}_in_{d}_setting\"\n",
    "        )\n",
    "\n",
    "    for layer in conv_layers:\n",
    "        # Visualize Feature Maps for class \"0\"\n",
    "        print(\"Visualizing feature maps for a sample of class '0':\")\n",
    "        if class_zero_sample is not None:\n",
    "            vgg.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_zero_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=0,\n",
    "                save_name=f\"VGG_{layer}_in_{d}_setting_for_zeros\"\n",
    "            )\n",
    "\n",
    "        # Visualize Feature Maps for class \"1\"\n",
    "        print(\"Visualizing feature maps for a sample of class '1':\")\n",
    "        if class_one_sample is not None:\n",
    "            vgg.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_one_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=1,\n",
    "                save_name=f\"VGG_{layer}_in_{d}_setting_for_ones\"\n",
    "            )\n",
    "\n",
    "    del(vgg)\n",
    "\n",
    "    resnet = ResNet(64, 1)\n",
    "    resnet_path = f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/models/resnet_20_epochs_0.001_lr_{d}.pth.tar\"\n",
    "    resnet.load_state_dict(torch.load(resnet_path))\n",
    "    resnet.eval()\n",
    "\n",
    "\n",
    "    # Visualize Filters\n",
    "    print(\"Visualizing filters of the first convolutional layer:\")\n",
    "    resnet.visualize_filters(\n",
    "            layer_name=conv_layers[0],\n",
    "            inputs=class_zero_sample,\n",
    "            num_filters=10,\n",
    "            save_name=f\"ResNet_{conv_layers[0]}_in_{d}_setting\"\n",
    "        )\n",
    "\n",
    "    for layer in conv_layers:\n",
    "        # Visualize Feature Maps for class \"0\"\n",
    "        print(\"Visualizing feature maps for a sample of class '0':\")\n",
    "        if class_zero_sample is not None:\n",
    "            resnet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_zero_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=0,\n",
    "                save_name=f\"ResNet_{layer}_in_{d}_setting_for_zeros\"\n",
    "            )\n",
    "\n",
    "        # Visualize Feature Maps for class \"1\"\n",
    "        print(\"Visualizing feature maps for a sample of class '1':\")\n",
    "        if class_one_sample is not None:\n",
    "            resnet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_one_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=1,\n",
    "                save_name=f\"ResNet_{layer}_in_{d}_setting_for_ones\"\n",
    "            )\n",
    "    \n",
    "    for layer in res_conv_layers:\n",
    "        # Visualize Feature Maps for class \"0\"\n",
    "        print(\"Visualizing feature maps for a sample of class '0':\")\n",
    "        if class_zero_sample is not None:\n",
    "            resnet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_zero_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=0,\n",
    "                save_name=f\"ResNet_{layer}_in_{d}_setting_for_zeros\"\n",
    "            )\n",
    "\n",
    "        # Visualize Feature Maps for class \"1\"\n",
    "        print(\"Visualizing feature maps for a sample of class '1':\")\n",
    "        if class_one_sample is not None:\n",
    "            resnet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_one_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=1,\n",
    "                save_name=f\"ResNet_{layer}_in_{d}_setting_for_ones\"\n",
    "            )\n",
    "\n",
    "    del(resnet)\n",
    "\n",
    "    unet = UNet(64)\n",
    "    unet_path = f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/models/unet_20_epochs_0.001_lr_{d}no_pretraining.pth.tar\"\n",
    "    unet.load_state_dict(torch.load(unet_path))\n",
    "    unet.eval()\n",
    "\n",
    "\n",
    "    # Visualize Filters\n",
    "    print(\"Visualizing filters of the first convolutional layer:\")\n",
    "    unet.visualize_filters(\n",
    "            layer_name=conv_layers[0],\n",
    "            inputs=class_zero_sample,\n",
    "            num_filters=10,\n",
    "            save_name=f\"unet_no_pretraining_{conv_layers[0]}_in_{d}_setting\"\n",
    "        )\n",
    "\n",
    "    for layer in conv_layers:\n",
    "        # Visualize Feature Maps for class \"0\"\n",
    "        print(\"Visualizing feature maps for a sample of class '0':\")\n",
    "        if class_zero_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_zero_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=0,\n",
    "                save_name=f\"unet_no_pretraining_{layer}_in_{d}_setting_for_zeros\"\n",
    "            )\n",
    "\n",
    "        # Visualize Feature Maps for class \"1\"\n",
    "        print(\"Visualizing feature maps for a sample of class '1':\")\n",
    "        if class_one_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_one_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=1,\n",
    "                save_name=f\"unet_no_pretraining_{layer}_in_{d}_setting_for_ones\"\n",
    "            )\n",
    "    \n",
    "    for layer in res_conv_layers:\n",
    "        # Visualize Feature Maps for class \"0\"\n",
    "        print(\"Visualizing feature maps for a sample of class '0':\")\n",
    "        if class_zero_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_zero_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=0,\n",
    "                save_name=f\"unet_no_pretraining_{layer}_in_{d}_setting_for_zeros\"\n",
    "            )\n",
    "\n",
    "        # Visualize Feature Maps for class \"1\"\n",
    "        print(\"Visualizing feature maps for a sample of class '1':\")\n",
    "        if class_one_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_one_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=1,\n",
    "                save_name=f\"unet_no_pretraining_{layer}_in_{d}_setting_for_ones\"\n",
    "            )\n",
    "\n",
    "    del(unet)\n",
    "\n",
    "    unet = UNet(64)\n",
    "    unet_path = f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/models/unet_20_epochs_0.001_lr_{d}_pretrained_resnet.pth.tar\"\n",
    "    unet.load_state_dict(torch.load(unet_path))\n",
    "    unet.eval()\n",
    "\n",
    "\n",
    "    # Visualize Filters\n",
    "    print(\"Visualizing filters of the first convolutional layer:\")\n",
    "    unet.visualize_filters(\n",
    "            layer_name=conv_layers[0],\n",
    "            inputs=class_zero_sample,\n",
    "            num_filters=10,\n",
    "            save_name=f\"unet_pretrained_{conv_layers[0]}_in_{d}_setting\"\n",
    "        )\n",
    "\n",
    "    for layer in conv_layers:\n",
    "        # Visualize Feature Maps for class \"0\"\n",
    "        print(\"Visualizing feature maps for a sample of class '0':\")\n",
    "        if class_zero_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_zero_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=0,\n",
    "                save_name=f\"unet_pretrained_{layer}_in_{d}_setting_for_zeros\"\n",
    "            )\n",
    "\n",
    "        # Visualize Feature Maps for class \"1\"\n",
    "        print(\"Visualizing feature maps for a sample of class '1':\")\n",
    "        if class_one_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_one_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=1,\n",
    "                save_name=f\"unet_pretrained_{layer}_in_{d}_setting_for_ones\"\n",
    "            )\n",
    "    \n",
    "    for layer in res_conv_layers:\n",
    "        # Visualize Feature Maps for class \"0\"\n",
    "        print(\"Visualizing feature maps for a sample of class '0':\")\n",
    "        if class_zero_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_zero_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=0,\n",
    "                save_name=f\"unet_pretrained_{layer}_in_{d}_setting_for_zeros\"\n",
    "            )\n",
    "\n",
    "        # Visualize Feature Maps for class \"1\"\n",
    "        print(\"Visualizing feature maps for a sample of class '1':\")\n",
    "        if class_one_sample is not None:\n",
    "            unet.visualize_feature_maps(\n",
    "                layer_name=layer,\n",
    "                inputs=class_one_sample,\n",
    "                num_maps=10,\n",
    "                relevance_based=True,\n",
    "                target_class=1,\n",
    "                save_name=f\"unet_pretrained_{layer}_in_{d}_setting_for_ones\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE latent space plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ClassComp.models.vae import VAE\n",
    "from ClassComp.data_utils.loaders import get_mnist, get_dataloader\n",
    "from ClassComp.utils.visualization import plot_vae_tsne_with_svm_boundary, plot_vae_outputs, plot_vae_samples, plot_vae_tsne_with_kmeans\n",
    "\n",
    "\n",
    "# Vanilla training\n",
    "\n",
    "image_size = 64\n",
    "batch_size = 16\n",
    "custom_transforms = None\n",
    "epochs = 2\n",
    "lr = 1e-3\n",
    "beta = 0.05\n",
    "descriptions = [\"vanilla\", \"noisy\"]\n",
    "\n",
    "## Get binary MNIST subsets\n",
    "train_subset, test_subset = get_mnist()\n",
    "\n",
    "## Get DataLoaders with additional transformations\n",
    "train_loader, test_loader = get_dataloader(train_subset, test_subset, transform=custom_transforms, size=image_size, batch_size=batch_size)\n",
    "\n",
    "\n",
    "vae = VAE(image_size**2, 32*32, 8, beta=beta)\n",
    "vae_path = f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/models/vae_20_epochs_0.001_lr_{d}_generation.pth.tar\"\n",
    "vae.load_state_dict(torch.load(vae_path))\n",
    "vae.eval()\n",
    "\n",
    "\n",
    "vae.classification_mode = \"KMeans\"\n",
    "\n",
    "with open(f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/models/VAE_{d}_kmeans.pkl\", \"rb\") as f:\n",
    "    kmeans = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "plot_vae_tsne_with_kmeans(\n",
    "    vae, \n",
    "    test_loader, \n",
    "    kmeans,\n",
    "    save_dir=f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/image/{d}_\",\n",
    ")\n",
    "vae.classification_mode = None\n",
    "plot_vae_tsne_with_svm_boundary(\n",
    "    vae, \n",
    "    test_loader,\n",
    "    save_path=f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/image/vae_tsne_with_svm_{d}.jpg\",\n",
    ")\n",
    "\n",
    "plot_vae_outputs(vae, test_loader, save_path=f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/image/vae_recreated_images_{d}.jpg\")\n",
    "\n",
    "plot_vae_samples(vae, save_path=f\"/home/leo/Programmation/Python/AML_project/ML_Model_Comparison/results/image/vae_sampled_images_{d}.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
